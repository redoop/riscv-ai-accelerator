"""
ç®€åŒ–çš„RISC-V AIè¿è¡Œæ—¶æ¨¡å— (macOSä»¿çœŸç‰ˆ)
"""

import torch
import torch.nn as nn
import time
import json
from typing import Dict, List, Tuple, Optional, Any, Union
import tempfile
import os
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„ä»¥å¯¼å…¥ riscv_ai_backend
sys.path.insert(0, str(Path(__file__).parent))

try:
    import riscv_ai_backend
    BACKEND_AVAILABLE = True
except ImportError:
    BACKEND_AVAILABLE = False

class RiscvAiRuntime:
    """RISC-V AIè¿è¡Œæ—¶"""
    
    def __init__(self, enable_profiling: bool = True):
        self.enable_profiling = enable_profiling
        self.models = {}
        self.performance_stats = {}
        
        if BACKEND_AVAILABLE:
            self.device_info = riscv_ai_backend.get_device_info()
        else:
            self.device_info = {"backend_available": False}
    
    def get_device_info(self) -> Dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        return self.device_info
    
    def load_model_from_torch(self, model: nn.Module, model_id: str, 
                            optimize: bool = True, sample_input: Optional[torch.Tensor] = None) -> str:
        """ä»PyTorchæ¨¡å‹åŠ è½½"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_id}")
        
        # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬åªæ˜¯ä¿å­˜æ¨¡å‹å¼•ç”¨
        self.models[model_id] = {
            "model": model,
            "optimized": optimize,
            "sample_input": sample_input,
            "loaded_at": time.time()
        }
        
        if optimize:
            print(f"âš¡ æ¨¡å‹ä¼˜åŒ–å·²å¯ç”¨ (ä»¿çœŸæ¨¡å¼)")
        
        return model_id
    
    def load_model(self, model_path: str, model_id: str, 
                  optimize: bool = True, sample_input: Optional[torch.Tensor] = None) -> str:
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½PyTorchæ¨¡å‹
        model = torch.load(model_path, map_location='cpu')
        model.eval()
        
        return self.load_model_from_torch(model, model_id, optimize, sample_input)
    
    def infer(self, model_id: str, input_data: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œæ¨ç†"""
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ°: {model_id}")
        
        model_info = self.models[model_id]
        model = model_info["model"]
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡Œæ¨ç†
        with torch.no_grad():
            if BACKEND_AVAILABLE and model_info["optimized"]:
                # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨åŸå§‹PyTorchæ¨¡å‹
                # ä½†ä¼šè®°å½•ä¸º"åŠ é€Ÿ"æ‰§è¡Œ
                output = model(input_data)
                # æ·»åŠ ä¸€äº›å»¶è¿Ÿæ¥æ¨¡æ‹ŸåŠ é€Ÿå™¨é€šä¿¡å¼€é”€
                time.sleep(0.001)  # 1mså»¶è¿Ÿ
            else:
                output = model(input_data)
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        end_time = time.time()
        exec_time = end_time - start_time
        
        if self.enable_profiling:
            if model_id not in self.performance_stats:
                self.performance_stats[model_id] = {
                    "total_inferences": 0,
                    "total_time": 0.0,
                    "min_time": float('inf'),
                    "max_time": 0.0
                }
            
            stats = self.performance_stats[model_id]
            stats["total_inferences"] += 1
            stats["total_time"] += exec_time
            stats["min_time"] = min(stats["min_time"], exec_time)
            stats["max_time"] = max(stats["max_time"], exec_time)
        
        return output
    
    def benchmark_model(self, model_id: str, input_shape: Tuple, 
                       num_iterations: int = 100, warmup_iterations: int = 10) -> Dict:
        """åŸºå‡†æµ‹è¯•æ¨¡å‹"""
        print(f"ğŸ åŸºå‡†æµ‹è¯•æ¨¡å‹: {model_id} ({num_iterations}æ¬¡è¿­ä»£)")
        
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ°: {model_id}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(*input_shape)
        
        # é¢„çƒ­
        print(f"ğŸ”¥ é¢„çƒ­ ({warmup_iterations}æ¬¡)...")
        for _ in range(warmup_iterations):
            _ = self.infer(model_id, test_input)
        
        # åŸºå‡†æµ‹è¯•
        print(f"â±ï¸  åŸºå‡†æµ‹è¯• ({num_iterations}æ¬¡)...")
        times = []
        
        for i in range(num_iterations):
            start_time = time.time()
            _ = self.infer(model_id, test_input)
            end_time = time.time()
            times.append(end_time - start_time)
            
            if (i + 1) % (num_iterations // 10) == 0:
                print(f"  è¿›åº¦: {i + 1}/{num_iterations}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        import statistics
        
        mean_time = statistics.mean(times)
        std_time = statistics.stdev(times) if len(times) > 1 else 0.0
        min_time = min(times)
        max_time = max(times)
        throughput = 1.0 / mean_time if mean_time > 0 else 0.0
        
        results = {
            "mean_time": mean_time,
            "std_time": std_time,
            "min_time": min_time,
            "max_time": max_time,
            "throughput": throughput,
            "total_iterations": num_iterations,
            "input_shape": input_shape
        }
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"  å¹³å‡æ—¶é—´: {mean_time:.6f}s")
        print(f"  æ ‡å‡†å·®: {std_time:.6f}s")
        print(f"  ååé‡: {throughput:.2f} inferences/sec")
        
        return results
    
    def get_performance_stats(self, model_id: str) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if model_id not in self.performance_stats:
            return {}
        
        stats = self.performance_stats[model_id].copy()
        
        if stats["total_inferences"] > 0:
            stats["average_time"] = stats["total_time"] / stats["total_inferences"]
            stats["throughput"] = stats["total_inferences"] / stats["total_time"]
        else:
            stats["average_time"] = 0.0
            stats["throughput"] = 0.0
        
        return stats
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºå·²åŠ è½½çš„æ¨¡å‹"""
        return list(self.models.keys())
    
    def get_model_info(self, model_id: str) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_id not in self.models:
            return {}
        
        model_info = self.models[model_id]
        model = model_info["model"]
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            "model_id": model_id,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "optimized": model_info["optimized"],
            "loaded_at": model_info["loaded_at"]
        }

def create_runtime(enable_profiling: bool = True) -> RiscvAiRuntime:
    """åˆ›å»ºè¿è¡Œæ—¶å®ä¾‹"""
    return RiscvAiRuntime(enable_profiling)
