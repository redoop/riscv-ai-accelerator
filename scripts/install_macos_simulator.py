#!/usr/bin/env python3
"""
macOS RISC-V AIä»¿çœŸå™¨å®‰è£…è„šæœ¬
å°†ä»¿çœŸå™¨é›†æˆåˆ°ç°æœ‰çš„PyTorchæµ‹è¯•æ¡†æ¶ä¸­
"""

import os
import sys
import shutil
import subprocess
from pathlib import Path
import tempfile

def create_riscv_ai_backend_module():
    """åˆ›å»ºriscv_ai_backendæ¨¡å—"""
    print("ğŸ“¦ åˆ›å»ºriscv_ai_backendæ¨¡å—...")
    
    # åˆ›å»ºæ¨¡å—ç›®å½•
    module_dir = Path("scripts/riscv_ai_backend")
    module_dir.mkdir(exist_ok=True)
    
    # åˆ›å»º__init__.pyæ–‡ä»¶
    init_content = '''"""
RISC-V AI Backend for macOS
æä¾›RISC-V AIåŠ é€Ÿå™¨çš„macOSä»¿çœŸæ”¯æŒ
"""

# å¯¼å…¥ä»¿çœŸåç«¯
from .riscv_ai_backend_macos import *

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0-macos-simulator"
__author__ = "RISC-V AI Team"
__description__ = "RISC-V AI Accelerator macOS Simulator"

# è‡ªåŠ¨åˆå§‹åŒ–æ ‡å¿—
_auto_initialize = True

def set_auto_initialize(enabled: bool):
    """è®¾ç½®æ˜¯å¦è‡ªåŠ¨åˆå§‹åŒ–"""
    global _auto_initialize
    _auto_initialize = enabled

# å°è¯•è‡ªåŠ¨åˆå§‹åŒ–
if _auto_initialize:
    try:
        initialize()
    except Exception as e:
        import warnings
        warnings.warn(f"è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
'''
    
    with open(module_dir / "__init__.py", "w") as f:
        f.write(init_content)
    
    # å¤åˆ¶ä»¿çœŸå™¨æ–‡ä»¶
    shutil.copy2("scripts/riscv_ai_backend_macos.py", module_dir / "riscv_ai_backend_macos.py")
    shutil.copy2("scripts/macos_ai_simulator.py", module_dir / "macos_ai_simulator.py")
    
    print(f"âœ… æ¨¡å—åˆ›å»ºå®Œæˆ: {module_dir}")
    return module_dir

def create_runtime_module():
    """åˆ›å»ºç®€åŒ–çš„runtimeæ¨¡å—"""
    print("ğŸƒ åˆ›å»ºruntimeæ¨¡å—...")
    
    runtime_content = '''"""
ç®€åŒ–çš„RISC-V AIè¿è¡Œæ—¶æ¨¡å— (macOSä»¿çœŸç‰ˆ)
"""

import torch
import torch.nn as nn
import time
import json
from typing import Dict, List, Tuple, Optional, Any, Union
import tempfile
import os

try:
    import sys
    sys.path.insert(0, str(Path(__file__).parent))
    import riscv_ai_backend
    BACKEND_AVAILABLE = True
except ImportError:
    BACKEND_AVAILABLE = False

class RiscvAiRuntime:
    """RISC-V AIè¿è¡Œæ—¶"""
    
    def __init__(self, enable_profiling: bool = True):
        self.enable_profiling = enable_profiling
        self.models = {}
        self.performance_stats = {}
        
        if BACKEND_AVAILABLE:
            self.device_info = riscv_ai_backend.get_device_info()
        else:
            self.device_info = {"backend_available": False}
    
    def get_device_info(self) -> Dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        return self.device_info
    
    def load_model_from_torch(self, model: nn.Module, model_id: str, 
                            optimize: bool = True, sample_input: Optional[torch.Tensor] = None) -> str:
        """ä»PyTorchæ¨¡å‹åŠ è½½"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_id}")
        
        # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬åªæ˜¯ä¿å­˜æ¨¡å‹å¼•ç”¨
        self.models[model_id] = {
            "model": model,
            "optimized": optimize,
            "sample_input": sample_input,
            "loaded_at": time.time()
        }
        
        if optimize:
            print(f"âš¡ æ¨¡å‹ä¼˜åŒ–å·²å¯ç”¨ (ä»¿çœŸæ¨¡å¼)")
        
        return model_id
    
    def load_model(self, model_path: str, model_id: str, 
                  optimize: bool = True, sample_input: Optional[torch.Tensor] = None) -> str:
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½PyTorchæ¨¡å‹
        model = torch.load(model_path, map_location='cpu')
        model.eval()
        
        return self.load_model_from_torch(model, model_id, optimize, sample_input)
    
    def infer(self, model_id: str, input_data: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œæ¨ç†"""
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ°: {model_id}")
        
        model_info = self.models[model_id]
        model = model_info["model"]
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡Œæ¨ç†
        with torch.no_grad():
            if BACKEND_AVAILABLE and model_info["optimized"]:
                # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨åŸå§‹PyTorchæ¨¡å‹
                # ä½†ä¼šè®°å½•ä¸º"åŠ é€Ÿ"æ‰§è¡Œ
                output = model(input_data)
                # æ·»åŠ ä¸€äº›å»¶è¿Ÿæ¥æ¨¡æ‹ŸåŠ é€Ÿå™¨é€šä¿¡å¼€é”€
                time.sleep(0.001)  # 1mså»¶è¿Ÿ
            else:
                output = model(input_data)
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        end_time = time.time()
        exec_time = end_time - start_time
        
        if self.enable_profiling:
            if model_id not in self.performance_stats:
                self.performance_stats[model_id] = {
                    "total_inferences": 0,
                    "total_time": 0.0,
                    "min_time": float('inf'),
                    "max_time": 0.0
                }
            
            stats = self.performance_stats[model_id]
            stats["total_inferences"] += 1
            stats["total_time"] += exec_time
            stats["min_time"] = min(stats["min_time"], exec_time)
            stats["max_time"] = max(stats["max_time"], exec_time)
        
        return output
    
    def benchmark_model(self, model_id: str, input_shape: Tuple, 
                       num_iterations: int = 100, warmup_iterations: int = 10) -> Dict:
        """åŸºå‡†æµ‹è¯•æ¨¡å‹"""
        print(f"ğŸ åŸºå‡†æµ‹è¯•æ¨¡å‹: {model_id} ({num_iterations}æ¬¡è¿­ä»£)")
        
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ°: {model_id}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(*input_shape)
        
        # é¢„çƒ­
        print(f"ğŸ”¥ é¢„çƒ­ ({warmup_iterations}æ¬¡)...")
        for _ in range(warmup_iterations):
            _ = self.infer(model_id, test_input)
        
        # åŸºå‡†æµ‹è¯•
        print(f"â±ï¸  åŸºå‡†æµ‹è¯• ({num_iterations}æ¬¡)...")
        times = []
        
        for i in range(num_iterations):
            start_time = time.time()
            _ = self.infer(model_id, test_input)
            end_time = time.time()
            times.append(end_time - start_time)
            
            if (i + 1) % (num_iterations // 10) == 0:
                print(f"  è¿›åº¦: {i + 1}/{num_iterations}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        import statistics
        
        mean_time = statistics.mean(times)
        std_time = statistics.stdev(times) if len(times) > 1 else 0.0
        min_time = min(times)
        max_time = max(times)
        throughput = 1.0 / mean_time if mean_time > 0 else 0.0
        
        results = {
            "mean_time": mean_time,
            "std_time": std_time,
            "min_time": min_time,
            "max_time": max_time,
            "throughput": throughput,
            "total_iterations": num_iterations,
            "input_shape": input_shape
        }
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"  å¹³å‡æ—¶é—´: {mean_time:.6f}s")
        print(f"  æ ‡å‡†å·®: {std_time:.6f}s")
        print(f"  ååé‡: {throughput:.2f} inferences/sec")
        
        return results
    
    def get_performance_stats(self, model_id: str) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if model_id not in self.performance_stats:
            return {}
        
        stats = self.performance_stats[model_id].copy()
        
        if stats["total_inferences"] > 0:
            stats["average_time"] = stats["total_time"] / stats["total_inferences"]
            stats["throughput"] = stats["total_inferences"] / stats["total_time"]
        else:
            stats["average_time"] = 0.0
            stats["throughput"] = 0.0
        
        return stats
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºå·²åŠ è½½çš„æ¨¡å‹"""
        return list(self.models.keys())
    
    def get_model_info(self, model_id: str) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_id not in self.models:
            return {}
        
        model_info = self.models[model_id]
        model = model_info["model"]
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            "model_id": model_id,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "optimized": model_info["optimized"],
            "loaded_at": model_info["loaded_at"]
        }

def create_runtime(enable_profiling: bool = True) -> RiscvAiRuntime:
    """åˆ›å»ºè¿è¡Œæ—¶å®ä¾‹"""
    return RiscvAiRuntime(enable_profiling)
'''
    
    with open("runtime.py", "w") as f:
        f.write(runtime_content)
    
    print("âœ… runtimeæ¨¡å—åˆ›å»ºå®Œæˆ")

def create_model_optimizer():
    """åˆ›å»ºæ¨¡å‹ä¼˜åŒ–å™¨æ¨¡å—"""
    print("âš¡ åˆ›å»ºmodel_optimizeræ¨¡å—...")
    
    optimizer_content = '''"""
RISC-V AIæ¨¡å‹ä¼˜åŒ–å™¨ (macOSä»¿çœŸç‰ˆ)
"""

import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional, Any

class RiscvAiOptimizer:
    """RISC-V AIæ¨¡å‹ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_levels = {
            "O0": "æ— ä¼˜åŒ–",
            "O1": "åŸºç¡€ä¼˜åŒ–", 
            "O2": "æ ‡å‡†ä¼˜åŒ–",
            "O3": "æ¿€è¿›ä¼˜åŒ–"
        }
    
    def optimize_model(self, model: nn.Module, sample_input: torch.Tensor, 
                      optimization_level: str = "O2") -> nn.Module:
        """ä¼˜åŒ–æ¨¡å‹"""
        print(f"âš¡ ä¼˜åŒ–æ¨¡å‹ (çº§åˆ«: {optimization_level})")
        
        if optimization_level not in self.optimization_levels:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–çº§åˆ«: {optimization_level}")
        
        # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¿”å›åŸå§‹æ¨¡å‹
        # å®é™…å®ç°ä¸­ä¼šè¿›è¡Œæ“ä½œèåˆã€å†…å­˜ä¼˜åŒ–ç­‰
        optimized_model = model
        
        print(f"âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ: {self.optimization_levels[optimization_level]}")
        
        return optimized_model

class RiscvAiQuantizer:
    """RISC-V AIé‡åŒ–å™¨"""
    
    def __init__(self):
        self.supported_schemes = ["int8", "int16", "fp16"]
    
    def quantize_model(self, model: nn.Module, calibration_loader, 
                      quantization_scheme: str = "int8") -> nn.Module:
        """é‡åŒ–æ¨¡å‹"""
        print(f"ğŸ”¢ é‡åŒ–æ¨¡å‹ (æ–¹æ¡ˆ: {quantization_scheme})")
        
        if quantization_scheme not in self.supported_schemes:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆ: {quantization_scheme}")
        
        # åœ¨ä»¿çœŸæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¿”å›åŸå§‹æ¨¡å‹
        # å®é™…å®ç°ä¸­ä¼šè¿›è¡Œæƒé‡é‡åŒ–
        quantized_model = model
        
        print(f"âœ… æ¨¡å‹é‡åŒ–å®Œæˆ: {quantization_scheme}")
        
        return quantized_model
'''
    
    with open("model_optimizer.py", "w") as f:
        f.write(optimizer_content)
    
    print("âœ… model_optimizeræ¨¡å—åˆ›å»ºå®Œæˆ")

def update_pytorch_test():
    """æ›´æ–°PyTorchæµ‹è¯•ç¨‹åºä»¥æ”¯æŒä»¿çœŸå™¨"""
    print("ğŸ”„ æ›´æ–°PyTorchæµ‹è¯•ç¨‹åº...")
    
    # è¯»å–ç°æœ‰çš„æµ‹è¯•ç¨‹åº
    test_file = "pytorch_chip_test.py"
    if not os.path.exists(test_file):
        print(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    with open(test_file, 'r') as f:
        content = f.read()
    
    # æ·»åŠ ä»¿çœŸå™¨æ”¯æŒçš„å¯¼å…¥è¯­å¥
    import_addition = '''
# macOSä»¿çœŸå™¨æ”¯æŒ
import platform
if platform.system() == "Darwin":
    print("ğŸ æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œå¯ç”¨ä»¿çœŸæ¨¡å¼")
    try:
        # å°è¯•å¯¼å…¥ä»¿çœŸå™¨
        import riscv_ai_backend
        print("âœ… RISC-V AIä»¿çœŸå™¨å·²åŠ è½½")
    except ImportError:
        print("âš ï¸  ä»¿çœŸå™¨æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: python3 install_macos_simulator.py")
'''
    
    # åœ¨å¯¼å…¥éƒ¨åˆ†åæ·»åŠ ä»¿çœŸå™¨æ”¯æŒ
    if "# macOSä»¿çœŸå™¨æ”¯æŒ" not in content:
        # æ‰¾åˆ°å¯¼å…¥éƒ¨åˆ†çš„ç»“æŸä½ç½®
        import_end = content.find('BACKEND_AVAILABLE = True')
        if import_end != -1:
            insert_pos = content.find('\n', import_end) + 1
            content = content[:insert_pos] + import_addition + content[insert_pos:]
            
            # å†™å›æ–‡ä»¶
            with open(test_file, 'w') as f:
                f.write(content)
            
            print("âœ… æµ‹è¯•ç¨‹åºå·²æ›´æ–°")
        else:
            print("âš ï¸  æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ’å…¥ä½ç½®")
    else:
        print("âœ… æµ‹è¯•ç¨‹åºå·²åŒ…å«ä»¿çœŸå™¨æ”¯æŒ")

def create_macos_specific_makefile():
    """åˆ›å»ºmacOSä¸“ç”¨çš„Makefileç›®æ ‡"""
    print("ğŸ“ æ›´æ–°Makefile...")
    
    makefile_addition = '''
# macOSä»¿çœŸå™¨æ”¯æŒ
.PHONY: install-simulator
install-simulator:
	@echo "ğŸ å®‰è£…macOS RISC-V AIä»¿çœŸå™¨..."
	$(PYTHON) install_macos_simulator.py
	@echo "âœ… ä»¿çœŸå™¨å®‰è£…å®Œæˆ"

.PHONY: test-simulator
test-simulator: install-simulator
	@echo "ğŸ§ª æµ‹è¯•ä»¿çœŸå™¨åŠŸèƒ½..."
	$(PYTHON) -c "import riscv_ai_backend; print('ä»¿çœŸå™¨ç‰ˆæœ¬:', riscv_ai_backend.__version__)"
	$(PYTHON) riscv_ai_backend_macos.py
	@echo "âœ… ä»¿çœŸå™¨æµ‹è¯•å®Œæˆ"

.PHONY: test-macos
test-macos: install-simulator test-simple
	@echo "ğŸ è¿è¡ŒmacOSå®Œæ•´æµ‹è¯•..."
	@if [ "$(shell uname -s)" = "Darwin" ]; then \
		echo "åœ¨macOSä¸Šè¿è¡Œä»¿çœŸæµ‹è¯•..."; \
		$(PYTHON) $(COMPREHENSIVE_TEST) --output $(OUTPUT_DIR)/macos_results.json \
			2>&1 | tee $(LOGS_DIR)/macos_test.log; \
	else \
		echo "âš ï¸  æ­¤ç›®æ ‡ä»…é€‚ç”¨äºmacOSç³»ç»Ÿ"; \
	fi

.PHONY: demo-simulator
demo-simulator: install-simulator
	@echo "ğŸ¬ è¿è¡Œä»¿çœŸå™¨æ¼”ç¤º..."
	$(PYTHON) -c "
import torch; \
import riscv_ai_backend as ai; \
print('ğŸš€ RISC-V AIä»¿çœŸå™¨æ¼”ç¤º'); \
print('è®¾å¤‡ä¿¡æ¯:', ai.get_device_info()); \
a = torch.randn(64, 64); \
b = torch.randn(64, 64); \
c = ai.mm(a, b); \
print('çŸ©é˜µä¹˜æ³•å®Œæˆ:', c.shape); \
print('æ€§èƒ½ç»Ÿè®¡:', ai.get_performance_stats()); \
"
'''
    
    makefile_path = "Makefile.pytorch_test"
    
    with open(makefile_path, 'r') as f:
        content = f.read()
    
    if "# macOSä»¿çœŸå™¨æ”¯æŒ" not in content:
        content += makefile_addition
        
        with open(makefile_path, 'w') as f:
            f.write(content)
        
        print("âœ… Makefileå·²æ›´æ–°")
    else:
        print("âœ… Makefileå·²åŒ…å«ä»¿çœŸå™¨æ”¯æŒ")

def main():
    """ä¸»å®‰è£…å‡½æ•°"""
    print("ğŸ RISC-V AIåŠ é€Ÿå™¨macOSä»¿çœŸå™¨å®‰è£…ç¨‹åº")
    print("=" * 50)
    
    try:
        # æ£€æŸ¥ç³»ç»Ÿ
        if os.uname().sysname != "Darwin":
            print("âš ï¸  æ­¤å®‰è£…ç¨‹åºä¸“ä¸ºmacOSè®¾è®¡")
            print("   åœ¨å…¶ä»–ç³»ç»Ÿä¸Šå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        
        # æ£€æŸ¥Pythonä¾èµ–
        print("ğŸ” æ£€æŸ¥ä¾èµ–...")
        try:
            import torch
            import numpy
            print("âœ… PyTorchå’ŒNumPyå·²å®‰è£…")
        except ImportError as e:
            print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            print("è¯·è¿è¡Œ: pip install torch numpy")
            return False
        
        # åˆ›å»ºæ¨¡å—
        module_dir = create_riscv_ai_backend_module()
        create_runtime_module()
        create_model_optimizer()
        
        # æ›´æ–°æµ‹è¯•ç¨‹åº
        update_pytorch_test()
        
        # æ›´æ–°Makefile
        create_macos_specific_makefile()
        
        print("\nğŸ‰ å®‰è£…å®Œæˆ!")
        print("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
        print("1. æµ‹è¯•ä»¿çœŸå™¨:")
        print("   make -f Makefile.pytorch_test test-simulator")
        print("\n2. è¿è¡Œæ¼”ç¤º:")
        print("   make -f Makefile.pytorch_test demo-simulator")
        print("\n3. è¿è¡Œå®Œæ•´æµ‹è¯•:")
        print("   make -f Makefile.pytorch_test test-macos")
        print("\n4. ç›´æ¥ä½¿ç”¨:")
        print("   python3 -c 'import sys; sys.path.insert(0, \"scripts\"); import riscv_ai_backend; print(riscv_ai_backend.get_device_info())'")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®‰è£…å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)