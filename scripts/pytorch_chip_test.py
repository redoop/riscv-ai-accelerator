#!/usr/bin/env python3
"""
RISC-V AIåŠ é€Ÿå™¨èŠ¯ç‰‡PyTorchç»¼åˆæµ‹è¯•ç¨‹åº
æµ‹è¯•TPUã€VPUå’ŒAIæŒ‡ä»¤æ‰©å±•çš„æ€§èƒ½å’ŒåŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import time
import sys
import os
from pathlib import Path
import argparse
import json
from typing import Dict, List, Tuple, Optional

# æ·»åŠ è½¯ä»¶æ¡†æ¶è·¯å¾„å’Œ scripts è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "software" / "frameworks" / "pytorch"))
sys.path.insert(0, str(Path(__file__).parent))

try:
    # macOSä»¿çœŸå™¨æ”¯æŒ
    import platform
    if platform.system() == "Darwin":
        print("ğŸ æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œå¯ç”¨ä»¿çœŸæ¨¡å¼")
    
    import riscv_ai_backend
    from model_optimizer import RiscvAiOptimizer, RiscvAiQuantizer
    from runtime import RiscvAiRuntime, create_runtime
    BACKEND_AVAILABLE = True
    print("âœ“ RISC-V AIåç«¯å¯ç”¨")
except ImportError as e:
    BACKEND_AVAILABLE = False
    print(f"âš  RISC-V AIåç«¯ä¸å¯ç”¨: {e}")
    print("å°†è¿è¡ŒCPUåŸºå‡†æµ‹è¯•")


class ChipTestSuite:
    """RISC-V AIåŠ é€Ÿå™¨èŠ¯ç‰‡æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, enable_profiling: bool = True):
        self.enable_profiling = enable_profiling
        self.results = {}
        
        if BACKEND_AVAILABLE:
            # åˆå§‹åŒ–RISC-V AIåç«¯
            riscv_ai_backend.initialize()
            self.runtime = create_runtime(enable_profiling=enable_profiling)
            self.optimizer = RiscvAiOptimizer()
            self.quantizer = RiscvAiQuantizer()
            
            # è·å–è®¾å¤‡ä¿¡æ¯
            self.device_info = self.runtime.get_device_info()
            print(f"âœ“ æ£€æµ‹åˆ° {self.device_info.get('tpu_count', 0)} ä¸ªTPU")
            print(f"âœ“ æ£€æµ‹åˆ° {self.device_info.get('vpu_count', 0)} ä¸ªVPU")
        else:
            self.runtime = None
            self.optimizer = None
            self.quantizer = None
            self.device_info = {}
    
    def test_basic_operations(self) -> Dict:
        """æµ‹è¯•åŸºæœ¬AIæ“ä½œ"""
        print("\n=== åŸºæœ¬æ“ä½œæµ‹è¯• ===")
        results = {}
        
        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        print("æµ‹è¯•çŸ©é˜µä¹˜æ³•...")
        sizes = [(64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)]
        
        for m, n in sizes:
            a = torch.randn(m, n, dtype=torch.float32)
            b = torch.randn(n, m, dtype=torch.float32)
            
            # CPUåŸºå‡†
            start_time = time.time()
            result_cpu = torch.mm(a, b)
            cpu_time = time.time() - start_time
            
            # RISC-V AIåŠ é€Ÿ
            if BACKEND_AVAILABLE:
                start_time = time.time()
                result_ai = riscv_ai_backend.mm(a, b)
                ai_time = time.time() - start_time
                
                # éªŒè¯æ­£ç¡®æ€§
                accuracy = torch.allclose(result_cpu, result_ai, rtol=1e-4, atol=1e-5)
                speedup = cpu_time / ai_time if ai_time > 0 else 0
                
                results[f"matmul_{m}x{n}"] = {
                    "cpu_time": cpu_time,
                    "ai_time": ai_time,
                    "speedup": speedup,
                    "accuracy": accuracy
                }
                
                print(f"  {m}x{n}: CPU={cpu_time:.4f}s, AI={ai_time:.4f}s, "
                      f"åŠ é€Ÿæ¯”={speedup:.2f}x, å‡†ç¡®æ€§={'âœ“' if accuracy else 'âœ—'}")
            else:
                results[f"matmul_{m}x{n}"] = {
                    "cpu_time": cpu_time,
                    "ai_time": None,
                    "speedup": None,
                    "accuracy": None
                }
                print(f"  {m}x{n}: CPU={cpu_time:.4f}s")
        
        # å·ç§¯æµ‹è¯•
        print("\næµ‹è¯•2Då·ç§¯...")
        conv_configs = [
            (1, 3, 32, 32, 16, 3),    # è¾“å…¥é€šé“, è¾“å‡ºé€šé“, é«˜, å®½, å·ç§¯æ ¸å¤§å°
            (1, 32, 64, 64, 64, 3),
            (1, 64, 128, 128, 128, 3),
        ]
        
        for batch, in_ch, h, w, out_ch, kernel in conv_configs:
            input_tensor = torch.randn(batch, in_ch, h, w, dtype=torch.float32)
            weight = torch.randn(out_ch, in_ch, kernel, kernel, dtype=torch.float32)
            bias = torch.randn(out_ch, dtype=torch.float32)
            
            # CPUåŸºå‡†
            start_time = time.time()
            result_cpu = F.conv2d(input_tensor, weight, bias, padding=1)
            cpu_time = time.time() - start_time
            
            # RISC-V AIåŠ é€Ÿ
            if BACKEND_AVAILABLE:
                start_time = time.time()
                result_ai = riscv_ai_backend.conv2d(input_tensor, weight, bias,
                                                   stride=[1, 1], padding=[1, 1],
                                                   dilation=[1, 1], groups=1)
                ai_time = time.time() - start_time
                
                accuracy = torch.allclose(result_cpu, result_ai, rtol=1e-3, atol=1e-4)
                speedup = cpu_time / ai_time if ai_time > 0 else 0
                
                results[f"conv2d_{in_ch}x{h}x{w}_{out_ch}"] = {
                    "cpu_time": cpu_time,
                    "ai_time": ai_time,
                    "speedup": speedup,
                    "accuracy": accuracy
                }
                
                print(f"  {in_ch}x{h}x{w}->{out_ch}: CPU={cpu_time:.4f}s, AI={ai_time:.4f}s, "
                      f"åŠ é€Ÿæ¯”={speedup:.2f}x, å‡†ç¡®æ€§={'âœ“' if accuracy else 'âœ—'}")
            else:
                results[f"conv2d_{in_ch}x{h}x{w}_{out_ch}"] = {
                    "cpu_time": cpu_time,
                    "ai_time": None,
                    "speedup": None,
                    "accuracy": None
                }
                print(f"  {in_ch}x{h}x{w}->{out_ch}: CPU={cpu_time:.4f}s")
        
        # æ¿€æ´»å‡½æ•°æµ‹è¯•
        print("\næµ‹è¯•æ¿€æ´»å‡½æ•°...")
        input_sizes = [1000, 10000, 100000]
        
        for size in input_sizes:
            input_tensor = torch.randn(size, dtype=torch.float32)
            
            # ReLUæµ‹è¯•
            start_time = time.time()
            result_cpu_relu = F.relu(input_tensor)
            cpu_relu_time = time.time() - start_time
            
            if BACKEND_AVAILABLE:
                start_time = time.time()
                result_ai_relu = riscv_ai_backend.relu(input_tensor)
                ai_relu_time = time.time() - start_time
                
                accuracy = torch.allclose(result_cpu_relu, result_ai_relu)
                speedup = cpu_relu_time / ai_relu_time if ai_relu_time > 0 else 0
                
                results[f"relu_{size}"] = {
                    "cpu_time": cpu_relu_time,
                    "ai_time": ai_relu_time,
                    "speedup": speedup,
                    "accuracy": accuracy
                }
                
                print(f"  ReLU({size}): CPU={cpu_relu_time:.6f}s, AI={ai_relu_time:.6f}s, "
                      f"åŠ é€Ÿæ¯”={speedup:.2f}x")
        
        return results
    
    def test_neural_networks(self) -> Dict:
        """æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹"""
        print("\n=== ç¥ç»ç½‘ç»œæ¨¡å‹æµ‹è¯• ===")
        results = {}
        
        # å®šä¹‰æµ‹è¯•æ¨¡å‹
        class SimpleNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
                self.bn1 = nn.BatchNorm2d(32)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
                self.bn2 = nn.BatchNorm2d(64)
                self.pool = nn.MaxPool2d(2, 2)
                self.fc1 = nn.Linear(64 * 8 * 8, 128)
                self.fc2 = nn.Linear(128, 10)
                self.relu = nn.ReLU()
                
            def forward(self, x):
                x = self.pool(self.relu(self.bn1(self.conv1(x))))
                x = self.pool(self.relu(self.bn2(self.conv2(x))))
                x = x.view(x.size(0), -1)
                x = self.relu(self.fc1(x))
                x = self.fc2(x)
                return x
        
        # åˆ›å»ºæ¨¡å‹å’Œè¾“å…¥
        model = SimpleNet()
        model.eval()
        sample_input = torch.randn(1, 3, 32, 32)
        
        # CPUåŸºå‡†æµ‹è¯•
        print("CPUåŸºå‡†æµ‹è¯•...")
        with torch.no_grad():
            start_time = time.time()
            for _ in range(100):
                output_cpu = model(sample_input)
            cpu_time = time.time() - start_time
        
        print(f"CPUæ¨ç†æ—¶é—´: {cpu_time:.4f}s (100æ¬¡)")
        
        if BACKEND_AVAILABLE:
            # åŠ è½½å’Œä¼˜åŒ–æ¨¡å‹
            print("åŠ è½½æ¨¡å‹åˆ°RISC-V AIè¿è¡Œæ—¶...")
            model_id = self.runtime.load_model_from_torch(model, "simple_net",
                                                        optimize=True, 
                                                        sample_input=sample_input)
            
            # AIåŠ é€Ÿæ¨ç†æµ‹è¯•
            print("RISC-V AIåŠ é€Ÿæ¨ç†æµ‹è¯•...")
            start_time = time.time()
            for _ in range(100):
                output_ai = self.runtime.infer(model_id, sample_input)
            ai_time = time.time() - start_time
            
            # éªŒè¯å‡†ç¡®æ€§
            accuracy = torch.allclose(output_cpu, output_ai, rtol=1e-3, atol=1e-4)
            speedup = cpu_time / ai_time if ai_time > 0 else 0
            
            print(f"AIæ¨ç†æ—¶é—´: {ai_time:.4f}s (100æ¬¡)")
            print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"å‡†ç¡®æ€§: {'âœ“' if accuracy else 'âœ—'}")
            
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            print("è¿è¡Œè¯¦ç»†æ€§èƒ½åŸºå‡†...")
            benchmark_stats = self.runtime.benchmark_model(model_id, (1, 3, 32, 32),
                                                         num_iterations=1000,
                                                         warmup_iterations=100)
            
            results["simple_net"] = {
                "cpu_time": cpu_time,
                "ai_time": ai_time,
                "speedup": speedup,
                "accuracy": accuracy,
                "benchmark_stats": benchmark_stats
            }
            
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {benchmark_stats['mean_time']:.6f}s")
            print(f"ååé‡: {benchmark_stats['throughput']:.2f} inferences/sec")
            print(f"æ ‡å‡†å·®: {benchmark_stats['std_time']:.6f}s")
        
        return results
    
    def test_quantization(self) -> Dict:
        """æµ‹è¯•é‡åŒ–åŠŸèƒ½"""
        print("\n=== é‡åŒ–æµ‹è¯• ===")
        results = {}
        
        if not BACKEND_AVAILABLE:
            print("è·³è¿‡é‡åŒ–æµ‹è¯• - RISC-V AIåç«¯ä¸å¯ç”¨")
            return results
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = models.resnet18(pretrained=False)
        model.eval()
        
        # åˆ›å»ºæ ¡å‡†æ•°æ®
        print("å‡†å¤‡æ ¡å‡†æ•°æ®...")
        calibration_data = []
        for _ in range(100):
            data = torch.randn(1, 3, 224, 224)
            target = torch.randint(0, 1000, (1,))
            calibration_data.append((data, target))
        
        calibration_loader = torch.utils.data.DataLoader(calibration_data, batch_size=1)
        
        # æµ‹è¯•ä¸åŒé‡åŒ–æ–¹æ¡ˆ
        quantization_schemes = ["int8", "int16"]
        
        for scheme in quantization_schemes:
            print(f"\næµ‹è¯•{scheme.upper()}é‡åŒ–...")
            
            try:
                # é‡åŒ–æ¨¡å‹
                quantized_model = self.quantizer.quantize_model(model, calibration_loader, scheme)
                
                # æµ‹è¯•æ¨ç†
                sample_input = torch.randn(1, 3, 224, 224)
                
                with torch.no_grad():
                    # åŸå§‹æ¨¡å‹
                    start_time = time.time()
                    output_fp32 = model(sample_input)
                    fp32_time = time.time() - start_time
                    
                    # é‡åŒ–æ¨¡å‹
                    start_time = time.time()
                    output_quantized = quantized_model(sample_input)
                    quantized_time = time.time() - start_time
                
                # è®¡ç®—å‡†ç¡®æ€§æŸå¤±
                accuracy_loss = torch.mean(torch.abs(output_fp32 - output_quantized)).item()
                speedup = fp32_time / quantized_time if quantized_time > 0 else 0
                
                results[f"quantization_{scheme}"] = {
                    "fp32_time": fp32_time,
                    "quantized_time": quantized_time,
                    "speedup": speedup,
                    "accuracy_loss": accuracy_loss
                }
                
                print(f"  FP32æ—¶é—´: {fp32_time:.6f}s")
                print(f"  {scheme.upper()}æ—¶é—´: {quantized_time:.6f}s")
                print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
                print(f"  å‡†ç¡®æ€§æŸå¤±: {accuracy_loss:.6f}")
                
            except Exception as e:
                print(f"  {scheme.upper()}é‡åŒ–å¤±è´¥: {e}")
                results[f"quantization_{scheme}"] = {"error": str(e)}
        
        return results
    
    def test_memory_performance(self) -> Dict:
        """æµ‹è¯•å†…å­˜æ€§èƒ½"""
        print("\n=== å†…å­˜æ€§èƒ½æµ‹è¯• ===")
        results = {}
        
        if not BACKEND_AVAILABLE:
            print("è·³è¿‡å†…å­˜æ€§èƒ½æµ‹è¯• - RISC-V AIåç«¯ä¸å¯ç”¨")
            return results
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„å†…å­˜åˆ†é…å’Œä¼ è¾“
        memory_sizes = [1, 10, 100, 1000]  # MB
        
        for size_mb in memory_sizes:
            size_bytes = size_mb * 1024 * 1024
            num_elements = size_bytes // 4  # float32
            
            print(f"æµ‹è¯• {size_mb}MB å†…å­˜ä¼ è¾“...")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            data = torch.randn(num_elements, dtype=torch.float32)
            
            # æµ‹è¯•å†…å­˜åˆ†é…
            start_time = time.time()
            device_data = riscv_ai_backend.allocate_memory(size_bytes)
            alloc_time = time.time() - start_time
            
            # æµ‹è¯•æ•°æ®ä¼ è¾“ (Host -> Device)
            start_time = time.time()
            riscv_ai_backend.copy_to_device(data, device_data)
            h2d_time = time.time() - start_time
            
            # æµ‹è¯•æ•°æ®ä¼ è¾“ (Device -> Host)
            start_time = time.time()
            result_data = riscv_ai_backend.copy_from_device(device_data, num_elements)
            d2h_time = time.time() - start_time
            
            # é‡Šæ”¾å†…å­˜
            riscv_ai_backend.free_memory(device_data)
            
            # è®¡ç®—å¸¦å®½
            h2d_bandwidth = size_mb / h2d_time if h2d_time > 0 else 0
            d2h_bandwidth = size_mb / d2h_time if d2h_time > 0 else 0
            
            results[f"memory_{size_mb}mb"] = {
                "alloc_time": alloc_time,
                "h2d_time": h2d_time,
                "d2h_time": d2h_time,
                "h2d_bandwidth_mbps": h2d_bandwidth,
                "d2h_bandwidth_mbps": d2h_bandwidth
            }
            
            print(f"  åˆ†é…æ—¶é—´: {alloc_time:.6f}s")
            print(f"  H2Dä¼ è¾“: {h2d_time:.6f}s ({h2d_bandwidth:.2f} MB/s)")
            print(f"  D2Hä¼ è¾“: {d2h_time:.6f}s ({d2h_bandwidth:.2f} MB/s)")
        
        return results
    
    def test_concurrent_execution(self) -> Dict:
        """æµ‹è¯•å¹¶å‘æ‰§è¡Œ"""
        print("\n=== å¹¶å‘æ‰§è¡Œæµ‹è¯• ===")
        results = {}
        
        if not BACKEND_AVAILABLE:
            print("è·³è¿‡å¹¶å‘æ‰§è¡Œæµ‹è¯• - RISC-V AIåç«¯ä¸å¯ç”¨")
            return results
        
        # æµ‹è¯•å¤šTPUå¹¶å‘
        tpu_count = self.device_info.get('tpu_count', 0)
        if tpu_count > 1:
            print(f"æµ‹è¯• {tpu_count} ä¸ªTPUå¹¶å‘æ‰§è¡Œ...")
            
            # åˆ›å»ºå¤šä¸ªçŸ©é˜µä¹˜æ³•ä»»åŠ¡
            matrices = []
            for i in range(tpu_count):
                a = torch.randn(512, 512, dtype=torch.float32)
                b = torch.randn(512, 512, dtype=torch.float32)
                matrices.append((a, b))
            
            # ä¸²è¡Œæ‰§è¡ŒåŸºå‡†
            start_time = time.time()
            for a, b in matrices:
                result = riscv_ai_backend.mm(a, b)
            serial_time = time.time() - start_time
            
            # å¹¶å‘æ‰§è¡Œ
            start_time = time.time()
            tasks = []
            for i, (a, b) in enumerate(matrices):
                task = riscv_ai_backend.mm_async(a, b, device_id=i % tpu_count)
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results_concurrent = []
            for task in tasks:
                result = riscv_ai_backend.wait_task(task)
                results_concurrent.append(result)
            
            concurrent_time = time.time() - start_time
            
            speedup = serial_time / concurrent_time if concurrent_time > 0 else 0
            
            results["concurrent_tpu"] = {
                "serial_time": serial_time,
                "concurrent_time": concurrent_time,
                "speedup": speedup,
                "tpu_count": tpu_count
            }
            
            print(f"  ä¸²è¡Œæ—¶é—´: {serial_time:.4f}s")
            print(f"  å¹¶å‘æ—¶é—´: {concurrent_time:.4f}s")
            print(f"  å¹¶å‘åŠ é€Ÿæ¯”: {speedup:.2f}x")
        else:
            print("åªæœ‰ä¸€ä¸ªTPUï¼Œè·³è¿‡å¹¶å‘æµ‹è¯•")
        
        return results
    
    def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("å¼€å§‹RISC-V AIåŠ é€Ÿå™¨èŠ¯ç‰‡ç»¼åˆæµ‹è¯•")
        print("=" * 50)
        
        all_results = {}
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        all_results["basic_operations"] = self.test_basic_operations()
        all_results["neural_networks"] = self.test_neural_networks()
        all_results["quantization"] = self.test_quantization()
        all_results["memory_performance"] = self.test_memory_performance()
        all_results["concurrent_execution"] = self.test_concurrent_execution()
        
        # æ·»åŠ è®¾å¤‡ä¿¡æ¯
        all_results["device_info"] = self.device_info
        all_results["backend_available"] = BACKEND_AVAILABLE
        
        return all_results
    
    def generate_report(self, results: Dict, output_file: Optional[str] = None):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        print("=" * 50)
        
        if BACKEND_AVAILABLE:
            print(f"âœ“ RISC-V AIåç«¯: å¯ç”¨")
            print(f"âœ“ TPUæ•°é‡: {self.device_info.get('tpu_count', 0)}")
            print(f"âœ“ VPUæ•°é‡: {self.device_info.get('vpu_count', 0)}")
        else:
            print("âš  RISC-V AIåç«¯: ä¸å¯ç”¨")
        
        # åŸºæœ¬æ“ä½œæ€§èƒ½æ‘˜è¦
        if "basic_operations" in results:
            print("\nåŸºæœ¬æ“ä½œæ€§èƒ½:")
            basic_ops = results["basic_operations"]
            
            # çŸ©é˜µä¹˜æ³•æ€§èƒ½
            matmul_speedups = [v["speedup"] for k, v in basic_ops.items() 
                             if k.startswith("matmul_") and v.get("speedup")]
            if matmul_speedups:
                avg_speedup = np.mean(matmul_speedups)
                max_speedup = np.max(matmul_speedups)
                print(f"  çŸ©é˜µä¹˜æ³•å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
                print(f"  çŸ©é˜µä¹˜æ³•æœ€å¤§åŠ é€Ÿæ¯”: {max_speedup:.2f}x")
            
            # å·ç§¯æ€§èƒ½
            conv_speedups = [v["speedup"] for k, v in basic_ops.items() 
                           if k.startswith("conv2d_") and v.get("speedup")]
            if conv_speedups:
                avg_speedup = np.mean(conv_speedups)
                max_speedup = np.max(conv_speedups)
                print(f"  å·ç§¯å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
                print(f"  å·ç§¯æœ€å¤§åŠ é€Ÿæ¯”: {max_speedup:.2f}x")
        
        # ç¥ç»ç½‘ç»œæ€§èƒ½
        if "neural_networks" in results and "simple_net" in results["neural_networks"]:
            nn_result = results["neural_networks"]["simple_net"]
            if nn_result.get("speedup"):
                print(f"\nç¥ç»ç½‘ç»œæ¨ç†åŠ é€Ÿæ¯”: {nn_result['speedup']:.2f}x")
                if "benchmark_stats" in nn_result:
                    stats = nn_result["benchmark_stats"]
                    print(f"æ¨ç†ååé‡: {stats['throughput']:.2f} inferences/sec")
        
        # é‡åŒ–æ€§èƒ½
        if "quantization" in results:
            print("\né‡åŒ–æ€§èƒ½:")
            for scheme in ["int8", "int16"]:
                key = f"quantization_{scheme}"
                if key in results["quantization"] and "speedup" in results["quantization"][key]:
                    speedup = results["quantization"][key]["speedup"]
                    loss = results["quantization"][key]["accuracy_loss"]
                    print(f"  {scheme.upper()}: åŠ é€Ÿæ¯”={speedup:.2f}x, å‡†ç¡®æ€§æŸå¤±={loss:.6f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        print("\næµ‹è¯•å®Œæˆ!")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if BACKEND_AVAILABLE:
            riscv_ai_backend.cleanup()


def main():
    parser = argparse.ArgumentParser(description="RISC-V AIåŠ é€Ÿå™¨èŠ¯ç‰‡PyTorchæµ‹è¯•ç¨‹åº")
    parser.add_argument("--output", "-o", type=str, default="test_results.json",
                       help="æµ‹è¯•ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--no-profiling", action="store_true",
                       help="ç¦ç”¨æ€§èƒ½åˆ†æ")
    parser.add_argument("--quick", action="store_true",
                       help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = ChipTestSuite(enable_profiling=not args.no_profiling)
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = test_suite.run_comprehensive_test()
        
        # ç”ŸæˆæŠ¥å‘Š
        test_suite.generate_report(results, args.output)
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        test_suite.cleanup()


if __name__ == "__main__":
    main()