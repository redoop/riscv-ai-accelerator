# TinyLlama 矩阵规模详解

## TinyLlama 1.1B 模型架构

### 基本参数
```
模型: TinyLlama-1.1B-Chat-v1.0
参数量: 1.1B (1,100,000,000)
层数: 22 layers
隐藏维度: 2048
中间维度: 5632
注意力头数: 32
每个头的维度: 64 (2048 / 32)
词汇表大小: 32000
```

## 推理时的矩阵规模

### 场景1: 单Token生成（最常见）
**输入形状**: `[batch=1, seq_len=1, hidden=2048]`

#### 1. 自注意力层 (Self-Attention)

**Q/K/V 投影矩阵:**
```
输入: [1, 1, 2048]
权重: [2048, 2048]
输出: [1, 1, 2048]

实际计算: 向量-矩阵乘法
[1, 2048] × [2048, 2048] = [1, 2048]

矩阵规模: 2048 × 2048
```

**注意力输出投影:**
```
输入: [1, 1, 2048]
权重: [2048, 2048]
输出: [1, 1, 2048]

矩阵规模: 2048 × 2048
```

**每层注意力总共:**
- Q投影: 2048 × 2048
- K投影: 2048 × 2048
- V投影: 2048 × 2048
- O投影: 2048 × 2048
- **总计: 4个 2048×2048 矩阵乘法**

#### 2. 前馈网络 (Feed-Forward Network)

**上投影 (Gate + Up):**
```
Gate投影:
输入: [1, 1, 2048]
权重: [2048, 5632]
输出: [1, 1, 5632]
矩阵规模: 2048 × 5632

Up投影:
输入: [1, 1, 2048]
权重: [2048, 5632]
输出: [1, 1, 5632]
矩阵规模: 2048 × 5632
```

**下投影:**
```
输入: [1, 1, 5632]
权重: [5632, 2048]
输出: [1, 1, 2048]

矩阵规模: 5632 × 2048
```

**每层FFN总共:**
- Gate投影: 2048 × 5632
- Up投影: 2048 × 5632
- Down投影: 5632 × 2048
- **总计: 3个大矩阵乘法**

### 场景2: Prompt处理（批量Token）
**输入形状**: `[batch=1, seq_len=N, hidden=2048]`

例如 seq_len=512:
```
注意力投影: [512, 2048] × [2048, 2048] = [512, 2048]
FFN上投影: [512, 2048] × [2048, 5632] = [512, 5632]
FFN下投影: [512, 5632] × [5632, 2048] = [512, 2048]
```

## 分块到 8×8 的计算

### 单层计算量分析

#### 注意力层 (2048×2048)

**分块数量:**
```
行方向: 2048 / 8 = 256 块
列方向: 2048 / 8 = 256 块
总块数: 256 × 256 = 65,536 个 8×8 矩阵

每个8×8矩阵: 8×8×8 = 512 次乘法 + 512 次加法 = 1024 次运算
总运算数: 65,536 × 1024 = 67,108,864 次运算
```

**计算时间（单个2048×2048矩阵）:**
```
假设 100MHz 时钟，532 周期/8×8矩阵:
时间 = 65,536 × 532 周期 / 100MHz
     = 34,865,152 周期 / 100,000,000 Hz
     = 0.349 秒
     = 349 毫秒
```

**单层注意力（4个2048×2048）:**
```
时间 = 4 × 349ms = 1,396 ms ≈ 1.4 秒
```

#### FFN层

**Gate投影 (2048×5632):**
```
分块: (2048/8) × (5632/8) = 256 × 704 = 180,224 个 8×8 矩阵
时间: 180,224 × 532 周期 / 100MHz = 958.8 ms
```

**Up投影 (2048×5632):**
```
同上: 958.8 ms
```

**Down投影 (5632×2048):**
```
分块: (5632/8) × (2048/8) = 704 × 256 = 180,224 个 8×8 矩阵
时间: 180,224 × 532 周期 / 100MHz = 958.8 ms
```

**单层FFN总时间:**
```
时间 = 958.8 + 958.8 + 958.8 = 2,876.4 ms ≈ 2.9 秒
```

### 单层总时间

```
注意力: 1.4 秒
FFN: 2.9 秒
LayerNorm等: ~0.1 秒
─────────────────
单层总计: 4.4 秒
```

### 完整模型推理

```
22 层 × 4.4 秒/层 = 96.8 秒/token

吞吐量: 1 / 96.8 = 0.01 tokens/秒
```

## 详细矩阵规模表

| 操作 | 输入形状 | 权重形状 | 输出形状 | 8×8块数 | 时间@100MHz |
|------|----------|----------|----------|---------|-------------|
| Q投影 | [1, 2048] | [2048, 2048] | [1, 2048] | 65,536 | 349 ms |
| K投影 | [1, 2048] | [2048, 2048] | [1, 2048] | 65,536 | 349 ms |
| V投影 | [1, 2048] | [2048, 2048] | [1, 2048] | 65,536 | 349 ms |
| O投影 | [1, 2048] | [2048, 2048] | [1, 2048] | 65,536 | 349 ms |
| Gate投影 | [1, 2048] | [2048, 5632] | [1, 5632] | 180,224 | 959 ms |
| Up投影 | [1, 2048] | [2048, 5632] | [1, 5632] | 180,224 | 959 ms |
| Down投影 | [1, 5632] | [5632, 2048] | [1, 2048] | 180,224 | 959 ms |

**单层总计:** 753,664 个 8×8 矩阵，4.4 秒

## 与其他模型对比

### 矩阵规模对比

| 模型 | Hidden Size | Intermediate | 注意力矩阵 | FFN矩阵 | 8×8块数/层 |
|------|-------------|--------------|------------|---------|------------|
| **TinyLlama** | **2048** | **5632** | **2048×2048** | **2048×5632** | **753,664** |
| Llama 2 7B | 4096 | 11008 | 4096×4096 | 4096×11008 | 3,014,656 |
| GPT-2 Small | 768 | 3072 | 768×768 | 768×3072 | 45,864 |
| DistilBERT | 768 | 3072 | 768×768 | 768×3072 | 45,864 |
| BERT Base | 768 | 3072 | 768×768 | 768×3072 | 45,864 |

### 性能对比

| 模型 | 单层时间 | 总层数 | Token延迟 | 可行性 |
|------|----------|--------|-----------|--------|
| GPT-2 Small | 0.24s | 12 | 2.9s | ⚠️ 勉强可用 |
| DistilBERT | 0.24s | 6 | 1.4s | ✅ 可用 |
| **TinyLlama** | **4.4s** | **22** | **96.8s** | ❌ 太慢 |
| Llama 2 7B | 17.6s | 32 | 563s | ❌ 完全不可行 |

## 优化后的性能估算

### 1. INT8 量化
```
内存带宽提升: 4倍
计算加速: 2倍
总加速: 4-6倍

TinyLlama INT8: 96.8s → 16-24s/token
仍然太慢 ❌
```

### 2. 稀疏化 (90%)
```
跳过零值计算
理论加速: 10倍
实际加速: 5-7倍

TinyLlama 稀疏: 96.8s → 14-19s/token
仍然太慢 ❌
```

### 3. 量化 + 稀疏化
```
组合加速: 20-30倍

TinyLlama 优化: 96.8s → 3-5s/token
勉强可用，但仍不理想 ⚠️
```

### 4. 多芯片并行 (4个芯片)
```
并行加速: 4倍

TinyLlama 4芯片: 96.8s → 24s/token
或 4芯片+优化: 3-5s → 0.75-1.25s/token
接近可用 ✅
```

## 结论

### TinyLlama 矩阵规模总结

**核心矩阵:**
- **2048 × 2048** (注意力层，4次/层)
- **2048 × 5632** (FFN上投影，2次/层)
- **5632 × 2048** (FFN下投影，1次/层)

**分块需求:**
- 单层需要 **753,664 个 8×8 矩阵乘法**
- 完整模型需要 **16,580,608 个 8×8 矩阵乘法**

**性能评估:**
- 单芯片: **96.8 秒/token** ❌ 不可用
- 优化后: **3-5 秒/token** ⚠️ 勉强可用（离线场景）
- 4芯片+优化: **0.75-1.25 秒/token** ✅ 可用

**建议:**
对于 TinyLlama，CompactScaleAiChip 单芯片不适合实时推理，但可以考虑：
1. 多芯片并行方案
2. 专注于更小的模型（<500M参数）
3. 离线批处理场景
